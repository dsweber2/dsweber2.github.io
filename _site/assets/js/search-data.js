var store = [{
        "title": "ScatteringTransform",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "/ScatteringTransform/"
      },{
        "title": "scattering",
        "excerpt":"The Scattering transform was my first point of contact with modern neuralnetwork theory, and is one of the oldest schools of thought. It came out of. So what are the claims? Scattering transform 2012 ","categories": [],
        "tags": [],
        "url": "/scattering/"
      },{
        "title": "Transformative experiences, shifting values and the modular mind",
        "excerpt":"inspired by rationally speaking 183 with L.A. Paul. combining the idea of themodular mind and semicoherent rationality. Rational agents should oppose changes in their values. Should nonrationalagents, or groups of rational agents? What about groups of rational agents whogain and lose members?   Do I contradict myself? Very well, then I contradict myself, I am large, I contain multitudes.   Walt Whitman A song of myself   To talk about thinking about multi-value agents, read some Elizabeth Anderson There are schools of thought which consider a person not as an agent with asingle, coherent utility function, but a whole panoply of agents, each withdifferent goals, with no guarantees that any of the agents last particularlylong. To my knowledge, these are not found frequently in the rationalistcommunity. Maybe this shouldn’t be surprising, given how starkly it flies inthe face of the base assumptions of homo economicus. Because I hate words, I’ll write agent aaa prefers AAA to BBB as A&gt;aBA&gt;_a BA&gt;a​B,every individual prefers AAA to BBB as A&gt;BA&gt;BA&gt;B, and the aggregate preference ofeveryone as ≻\\succ≻. What can we say about what a group of agents should do? This is, at core a question of how to integrate multiple utility functions, whichruns  Arrow’stheorem.. Roughly,we must break at least one of the following:   If for each person, X&gt;YX&gt;YX&gt;Y, then X≻YX \\succ YX≻Y, i.e. everyone prefers XXX toYYY, then the aggregator prefers XXX to YYY. I should hope such a criteriais obviously essential for any aggregator purporting to represent any of itsconstituent agents.  Independence of irrelevant alternatives (iira). Suppose X≻YX\\succ YX≻Y. Thensuppose some subset of people bbb change their mind on the relative merits ofXXX and ZZZ. One would hope that X≻YX\\succ YX≻Y. This is the axiom that is mostfrequently held as too strong, as there are cases which arerock-paper-scissors-like, and thus necessarily tying together more than twooptions.  No one person’s vote determines the outcome, regardless of the other agents.Turns out, we can’t actually have all of these. Probably the most reasonableconclusion from Arrow’s theorem is that any voting scheme with more than twooptions and more than 4 people is gamable, so you should vote strategically. Toaccount for this I’m going to include a 3rd preference indication, X≫aYX\\gg_aYX≫a​Y,which means that aaa truly prefers XXX to YYY, and is not just strategicallyvoting as such, which is what X&gt;aYX&gt;_a YX&gt;a​Y means. What I want to focus on here isn’t the traditional public choice theoryquestions. I want to think about in what ways does a collective diverge frombehaving like an agent? The most obvious possibility is that iira can cause theagent to not behave consistently. Should we expect value drift over time? The case that got me started down this road was thinking about this rationallyspeaking with LAPaulabout transformative life experiences and changing your values. It is anobvious truism that a fully rational agent has no interest in modifying it’sutility function; that would, by definition, lower it’s objectivefunction. There’s more room for boundedly rational agents to have layers ofvalues, some of which are malleable, and controlled by the higher ones. What about in the context of the, for lack of a better term, aggregated mind?In this case, there are two real ways you can modify the aggregatorpreferences. One is by causing the strategic votes cast by the sub-agents tochange, be it through mixed equilibria, or through switching between equilibriaby some means (more later). The other, which reflects the actual situationabove somewhat better, is by the introduction of new agents. Increase the Horde Will a collection of agents vote to allow new agents? There’s still some partsof this set-up that are vague. For example, without other votes beyond theinclusion of a new agent, there is no reason for any agent to have preferencesin that election. We will assume that the vote to include new agents ispurely strategic in terms of what will happen in the other games. In thatcase, any agent will obviously prefer to add a voter with the same preferencesas their own. Another question is whether there is a choice of how many or whatkind of agent to add. If the question is binary, then the concerns of Arrow’stheorem mostly go away. On the other hand, one might guess that if the agentsare allowed to vote on what the new agent looks like, or allowed to choose toadd multiple new agents, they would be more inclined to add new agents. So let’s say we’ve just got the inclusion vote and another vote. Further, tokeep it simple, lets consider the excessively gamable first past the postvoting scheme. Game theoretic preference shifting ","categories": ["rationality","altruism"],
        "tags": [],
        "url": "/rationality/altruism/Multiagents/"
      }]
